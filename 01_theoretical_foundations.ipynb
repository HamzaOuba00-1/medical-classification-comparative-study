{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac35a4e",
   "metadata": {},
   "source": [
    "# Theoretical Foundations of Classification Algorithms for Medical Decision Support\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Medical decision support systems increasingly rely on **machine learning classification algorithms** to assist clinicians in diagnosis, prognosis, and risk assessment.  \n",
    "In such systems, models must not only achieve high predictive performance, but also satisfy important theoretical requirements such as **stability**, **generalization ability**, **robustness to class imbalance**, and **interpretability**.\n",
    "\n",
    "This notebook presents the **theoretical foundations** of three widely used classification algorithms applied in this project:\n",
    "\n",
    "- **Logistic Regression**  \n",
    "- **Support Vector Machine (SVM) with Radial Basis Function (RBF) kernel**  \n",
    "- **Random Forest**\n",
    "\n",
    "These models were selected because they represent **complementary learning paradigms**:\n",
    "- probabilistic linear models,\n",
    "- margin-based kernel methods,\n",
    "- and ensemble-based decision tree approaches.\n",
    "\n",
    "For each algorithm, we focus on:\n",
    "- the **mathematical formulation** of the model,\n",
    "- the **optimization objective** and learning process,\n",
    "- the **statistical interpretation** (when applicable),\n",
    "- and the **theoretical properties** that make the method suitable for clinical decision support.\n",
    "\n",
    "This theoretical analysis provides the foundation for the experimental evaluation conducted in the accompanying notebooks:\n",
    "- one dedicated to a **small clinical dataset**,  \n",
    "- and another to a **large-scale clinical dataset**.\n",
    "\n",
    "By clearly separating **theory** from **implementation**, this notebook aims to justify the modeling choices and to facilitate a principled comparison of classification algorithms in a medical context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480de97",
   "metadata": {},
   "source": [
    "### **1. Logistic Regression — mathematical background**\n",
    "\n",
    "Logistic Regression is a **probabilistic linear classifier** designed for binary classification problems.  \n",
    "Unlike linear regression, it models the **probability** of class membership rather than a continuous output.\n",
    "\n",
    "Given an input vector $x$, the model computes a linear score:\n",
    "\n",
    "$$\n",
    "z = w^\\top x + b\n",
    "$$\n",
    "\n",
    "which is then transformed using the **sigmoid (logistic) function**:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}.\n",
    "$$\n",
    "\n",
    "The resulting hypothesis is:\n",
    "\n",
    "$$\n",
    "h_w(x) = \\sigma(w^\\top x + b),\n",
    "$$\n",
    "\n",
    "which can be interpreted as the conditional probability:\n",
    "\n",
    "$$\n",
    "h_w(x) = P(y = 1 \\mid x; w).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Decision rule\n",
    "\n",
    "Predictions are obtained by thresholding the estimated probability:\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } h_w(x) \\ge 0.5, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since $\\sigma(z) \\ge 0.5 \\iff z \\ge 0$, the decision boundary is given by:\n",
    "\n",
    "$$\n",
    "w^\\top x + b = 0,\n",
    "$$\n",
    "\n",
    "which corresponds to a **linear separating hyperplane** in the feature space.  \n",
    "Non-linear decision boundaries can be obtained by introducing non-linear feature mappings.\n",
    "\n",
    "---\n",
    "\n",
    "#### Loss function and convexity\n",
    "\n",
    "Because classification targets are binary, squared error loss is inappropriate.  \n",
    "Logistic Regression instead minimises the **logistic loss**, also known as **binary cross-entropy**.\n",
    "\n",
    "For a single sample $(x_i, y_i)$, the loss is defined as:\n",
    "\n",
    "$$\n",
    "\\ell(h_w(x_i), y_i)\n",
    "=\n",
    "- y_i \\log(h_w(x_i))\n",
    "- (1 - y_i)\\log(1 - h_w(x_i)).\n",
    "$$\n",
    "\n",
    "The empirical risk over the dataset is:\n",
    "\n",
    "$$\n",
    "J(w) =\n",
    "-\\frac{1}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "\\Big[\n",
    "y_i \\log(h_w(x_i))\n",
    "+\n",
    "(1 - y_i)\\log(1 - h_w(x_i))\n",
    "\\Big].\n",
    "$$\n",
    "\n",
    "This objective function is **convex**, guaranteeing a unique global optimum and stable convergence.\n",
    "\n",
    "---\n",
    "\n",
    "#### Statistical interpretation: Maximum Likelihood Estimation\n",
    "\n",
    "Logistic Regression has a strong statistical foundation.  \n",
    "The target variable is assumed to follow a **Bernoulli distribution**:\n",
    "\n",
    "$$\n",
    "y_i \\sim \\text{Bernoulli}(p_i),\n",
    "\\quad\n",
    "p_i = h_w(x_i).\n",
    "$$\n",
    "\n",
    "The likelihood function is:\n",
    "\n",
    "$$\n",
    "L(w) =\n",
    "\\prod_{i=1}^{n}\n",
    "p_i^{y_i}\n",
    "(1 - p_i)^{1 - y_i}.\n",
    "$$\n",
    "\n",
    "Taking the logarithm yields the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\log L(w) =\n",
    "\\sum_{i=1}^{n}\n",
    "\\Big[\n",
    "y_i \\log(p_i)\n",
    "+\n",
    "(1 - y_i)\\log(1 - p_i)\n",
    "\\Big].\n",
    "$$\n",
    "\n",
    "Minimising the binary cross-entropy loss is therefore equivalent to **maximising the log-likelihood**, meaning that Logistic Regression parameters are estimated using **Maximum Likelihood Estimation (MLE)**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Optimisation\n",
    "\n",
    "Model parameters are typically learned using **gradient descent**.  \n",
    "The gradient of the cost function with respect to parameter $w_j$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(w)}{\\partial w_j}\n",
    "=\n",
    "\\frac{1}{n}\n",
    "\\sum_{i=1}^{n}\n",
    "\\big(h_w(x_i) - y_i\\big) x_{ij}.\n",
    "$$\n",
    "\n",
    "The update rule is:\n",
    "\n",
    "$$\n",
    "w_j \\leftarrow w_j - \\alpha\n",
    "\\sum_{i=1}^{n}\n",
    "\\big(h_w(x_i) - y_i\\big) x_{ij},\n",
    "$$\n",
    "\n",
    "where $\\alpha > 0$ is the learning rate.\n",
    "\n",
    "Thanks to the sigmoid function, gradients are **smooth and bounded**, which improves numerical stability compared to vanilla linear classifiers.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe29796",
   "metadata": {},
   "source": [
    "\n",
    "### **2. Support Vector Machine with RBF kernel — mathematical background**\n",
    "\n",
    "A Support Vector Machine (SVM) is a **maximum-margin classifier** for binary classification, where class labels are defined as\n",
    "\n",
    "$$\n",
    "y_i \\in \\{-1, +1\\}.\n",
    "$$\n",
    "\n",
    "The objective of SVM is to find a separating hyperplane that maximises the **margin**, defined as the distance between the decision boundary and the closest training samples from each class (the *support vectors*).\n",
    "\n",
    "In the linear case, the decision boundary is defined as:\n",
    "\n",
    "$$\n",
    "w^\\top x + b = 0.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Maximal margin and hard-margin formulation\n",
    "\n",
    "For linearly separable data, SVM enforces the constraints:\n",
    "\n",
    "$$\n",
    "y_i (w^\\top x_i + b) \\ge 1,\n",
    "\\qquad i = 1, \\dots, n.\n",
    "$$\n",
    "\n",
    "The margin is given by:\n",
    "\n",
    "$$\n",
    "\\rho = \\frac{2}{\\lVert w \\rVert}.\n",
    "$$\n",
    "\n",
    "Maximising the margin is equivalent to minimising $\\lVert w \\rVert$, which leads to the **hard-margin SVM** optimisation problem:\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\; \\frac{1}{2} \\lVert w \\rVert^2\n",
    "\\quad \\text{subject to} \\quad\n",
    "y_i (w^\\top x_i + b) \\ge 1.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Soft-margin SVM\n",
    "\n",
    "In practice, real-world data (including medical data) are rarely perfectly separable.  \n",
    "To allow violations of the margin constraints, SVM introduces **slack variables** $\\xi_i \\ge 0$, resulting in the **soft-margin SVM** formulation:\n",
    "\n",
    "$$\n",
    "\\min_{w, b, \\{\\xi_i\\}}\n",
    "\\; \\frac{1}{2} \\lVert w \\rVert^2\n",
    "\\;+\\;\n",
    "C \\sum_{i=1}^{n} \\xi_i\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "y_i (w^\\top x_i + b) \\ge 1 - \\xi_i,\n",
    "\\qquad\n",
    "\\xi_i \\ge 0,\n",
    "\\qquad\n",
    "i = 1, \\dots, n.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $\\xi_i$ quantify margin violations and misclassifications,\n",
    "- $C > 0$ is a regularisation parameter controlling the trade-off between **margin maximisation** and **classification error**:\n",
    "  - large $C$: low tolerance to errors (narrower margin, lower bias, higher variance),\n",
    "  - small $C$: higher tolerance to errors (wider margin, higher bias, lower variance).\n",
    "\n",
    "This formulation corresponds to minimising a regularised **hinge loss** objective.\n",
    "\n",
    "---\n",
    "\n",
    "#### Kernel trick and non-linear decision boundaries\n",
    "\n",
    "Linear SVMs cannot separate data with non-linear structure.  \n",
    "To address this, SVMs use the **kernel trick**, which implicitly maps inputs into a higher-dimensional feature space:\n",
    "\n",
    "$$\n",
    "\\phi : \\mathbb{R}^d \\longrightarrow \\mathcal{H}.\n",
    "$$\n",
    "\n",
    "Instead of computing $\\phi(x)$ explicitly, SVMs rely only on inner products in the feature space, defined via a kernel function:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\phi(x_i)^\\top \\phi(x_j).\n",
    "$$\n",
    "\n",
    "This allows SVMs to learn non-linear decision boundaries while keeping the optimisation problem computationally tractable.\n",
    "\n",
    "---\n",
    "\n",
    "#### Radial Basis Function (RBF) kernel\n",
    "\n",
    "In this project, we use the **Radial Basis Function (RBF) kernel**, defined as:\n",
    "\n",
    "$$\n",
    "K(x_i, x_j) = \\exp\\!\\left(-\\gamma \\lVert x_i - x_j \\rVert^2\\right).\n",
    "$$\n",
    "\n",
    "The RBF kernel corresponds to an **infinite-dimensional feature space** and produces smooth, flexible decision boundaries.\n",
    "\n",
    "The parameter $\\gamma > 0$ controls the locality of the kernel:\n",
    "\n",
    "- large $\\gamma$: very local influence of each support vector, leading to highly flexible boundaries and potential overfitting,\n",
    "- small $\\gamma$: smoother, more global boundaries with stronger regularisation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Dual formulation and support vectors\n",
    "\n",
    "Using kernels, SVM optimisation is performed in the **dual formulation**, and the decision function becomes:\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\operatorname{sign}\n",
    "\\left(\n",
    "b + \\sum_{i \\in \\mathcal{S}} \\alpha_i y_i K(x, x_i)\n",
    "\\right),\n",
    "$$\n",
    "\n",
    "where only training samples with non-zero $\\alpha_i$ contribute to the decision.  \n",
    "These samples are called **support vectors**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Practical implementation\n",
    "\n",
    "In `scikit-learn`:\n",
    "\n",
    "- `C` controls the soft-margin regularisation,\n",
    "- `gamma` defines the RBF kernel width,\n",
    "- `probability=True` enables probability estimation via **Platt scaling**.\n",
    "\n",
    "Hyperparameters $(C, \\gamma)$ are selected using `GridSearchCV`, which evaluates candidate models and retains the combination that maximises the **cross-validated ROC-AUC**, a metric particularly suited to imbalanced medical datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27df4e",
   "metadata": {},
   "source": [
    "### **3. Random Forest — mathematical background**\n",
    "\n",
    "\n",
    "## 1 Training data (Bootstrap sampling)\n",
    "\n",
    "Given a dataset:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^{n}\n",
    "$$\n",
    "\n",
    "For each tree \\( t = 1, \\dots, T \\), sample **with replacement**:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_t \\sim \\text{Bootstrap}(\\mathcal{D})\n",
    "$$\n",
    "\n",
    " Each tree is trained on a **different dataset**\n",
    "\n",
    "---\n",
    "\n",
    "## 2️ Decision tree split (feature randomness)\n",
    "\n",
    "At each node:\n",
    "\n",
    "- Randomly select a subset of features:\n",
    "\n",
    "$$\n",
    "\\mathcal{F}_t \\subset \\{1,\\dots,p\\}, \\quad |\\mathcal{F}_t| = m \\ll p\n",
    "$$\n",
    "\n",
    "- Choose the best split by minimizing impurity:\n",
    "\n",
    "$$\n",
    "\\arg\\min_{\\text{split}} \\; \\text{Impurity}\n",
    "$$\n",
    "\n",
    "### Common impurity measures\n",
    "\n",
    "#### Gini impurity\n",
    "\n",
    "$$\n",
    "G = 1 - \\sum_{k=1}^{K} p_k^2\n",
    "$$\n",
    "\n",
    "#### Entropy\n",
    "\n",
    "$$\n",
    "H = - \\sum_{k=1}^{K} p_k \\log p_k\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3️ Tree prediction\n",
    "\n",
    "Each tree produces a prediction:\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(t)}(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4️ Forest prediction (aggregation)\n",
    "\n",
    "### Classification (majority vote)\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) =\n",
    "\\arg\\max_{k}\n",
    "\\sum_{t=1}^{T}\n",
    "\\mathbf{1}\\big(\\hat{y}^{(t)}(x) = k\\big)\n",
    "$$\n",
    "\n",
    "### Regression (average)\n",
    "\n",
    "$$\n",
    "\\hat{y}(x) =\n",
    "\\frac{1}{T}\n",
    "\\sum_{t=1}^{T}\n",
    "\\hat{y}^{(t)}(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5️ Why Random Forest works (math intuition)\n",
    "\n",
    "- Each decision tree is a **high-variance estimator**\n",
    "- Averaging reduces variance:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}\\!\\left(\\frac{1}{T}\\sum_{t=1}^{T} f_t(x)\\right)\n",
    "\\;\\downarrow\n",
    "$$\n",
    "\n",
    "- Randomness reduces correlation between trees\n",
    "- Result: **low variance and strong generalization**\n",
    "\n",
    "---\n",
    "\n",
    "## 6️ Bias–Variance perspective\n",
    "\n",
    "- Single decision tree → low bias, high variance  \n",
    "- Random Forest → slightly higher bias, **much lower variance**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Random Forest is an **ensemble learning method** for classification and regression that combines multiple decision trees to improve generalisation and robustness.  \n",
    "The core idea is to reduce variance by aggregating the predictions of many weakly correlated models.\n",
    "\n",
    "---\n",
    "\n",
    "#### Decision trees as base learners\n",
    "\n",
    "A Random Forest is built upon **decision trees**, which recursively partition the feature space.  \n",
    "At each internal node, a split is selected by maximising a purity criterion.\n",
    "\n",
    "For classification, the most common impurity measure is the **Gini impurity**, defined for a node containing samples from $K$ classes as:\n",
    "\n",
    "$$\n",
    "\\text{Gini} = 1 - \\sum_{k=1}^{K} p_k^2,\n",
    "$$\n",
    "\n",
    "where $p_k$ is the proportion of samples belonging to class $k$ in the node.\n",
    "\n",
    "Alternatively, **entropy** may be used:\n",
    "\n",
    "$$\n",
    "\\text{Entropy} = - \\sum_{k=1}^{K} p_k \\log(p_k).\n",
    "$$\n",
    "\n",
    "A split is chosen to maximise the **information gain**, i.e. the reduction in impurity after splitting.\n",
    "\n",
    "---\n",
    "\n",
    "#### Bootstrap aggregation (bagging)\n",
    "\n",
    "Random Forest applies **bootstrap aggregation (bagging)** to decision trees.\n",
    "\n",
    "Given a training dataset of size $n$, each tree is trained on a **bootstrap sample** obtained by sampling $n$ observations **with replacement** from the original dataset.\n",
    "\n",
    "As a result:\n",
    "- different trees are trained on different subsets of the data,\n",
    "- approximately one-third of the samples are left out of each bootstrap sample (*out-of-bag samples*).\n",
    "\n",
    "Bagging reduces variance by averaging multiple high-variance estimators.\n",
    "\n",
    "---\n",
    "\n",
    "#### Random feature selection\n",
    "\n",
    "To further decorrelate the trees, Random Forest introduces **random feature selection**.\n",
    "\n",
    "At each split, instead of considering all $m$ input features, only a random subset of size $m_{\\text{try}}$ is evaluated.\n",
    "\n",
    "For classification, a common choice is:\n",
    "\n",
    "$$\n",
    "m_{\\text{try}} = \\sqrt{m}.\n",
    "$$\n",
    "\n",
    "This randomness forces trees to explore different structures, reducing correlation between trees and improving ensemble performance.\n",
    "\n",
    "---\n",
    "\n",
    "#### Ensemble prediction\n",
    "\n",
    "Let $\\{h_1(x), h_2(x), \\dots, h_T(x)\\}$ denote the predictions of the $T$ individual trees.\n",
    "\n",
    "For **classification**, the Random Forest prediction is obtained by **majority voting**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname{mode}\\{h_t(x)\\}_{t=1}^{T}.\n",
    "$$\n",
    "\n",
    "For **probabilistic outputs**, class probabilities are estimated by averaging the class probabilities predicted by individual trees.\n",
    "\n",
    "---\n",
    "\n",
    "#### Bias–variance trade-off\n",
    "\n",
    "Decision trees are low-bias but high-variance models.  \n",
    "Random Forest reduces variance through averaging while maintaining low bias.\n",
    "\n",
    "Increasing the number of trees $T$:\n",
    "- does **not** increase overfitting,\n",
    "- improves stability,\n",
    "- converges to a limiting generalisation error.\n",
    "\n",
    "---\n",
    "\n",
    "#### Out-of-bag (OOB) error estimation\n",
    "\n",
    "Because each tree is trained on a bootstrap sample, predictions can be evaluated on the samples not used during training.\n",
    "\n",
    "The **out-of-bag error** provides an unbiased estimate of the generalisation error without requiring a separate validation set.\n",
    "\n",
    "---\n",
    "\n",
    "#### Theoretical particularities\n",
    "\n",
    "Random Forest exhibits several important theoretical properties:\n",
    "\n",
    "- Strong resistance to overfitting due to ensemble averaging  \n",
    "- Ability to model complex non-linear decision boundaries  \n",
    "- Invariance to monotonic feature transformations  \n",
    "- Robustness to noise and outliers  \n",
    "\n",
    "Additionally, Random Forest provides **feature importance measures**, typically based on:\n",
    "- mean decrease in impurity, or\n",
    "- permutation importance.\n",
    "\n",
    "---\n",
    "\n",
    "#### Practical implementation\n",
    "\n",
    "In `scikit-learn`, the main hyperparameters include:\n",
    "\n",
    "- `n_estimators`: number of trees in the forest,\n",
    "- `max_depth`: maximum depth of each tree,\n",
    "- `min_samples_split`, `min_samples_leaf`: control tree complexity,\n",
    "- `max_features`: number of features considered at each split.\n",
    "\n",
    "In this project, hyperparameters are selected using `GridSearchCV`, and model performance is evaluated using **ROC-AUC**, which is well suited to imbalanced medical datasets.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
